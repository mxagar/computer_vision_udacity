# Udacity Computer Vision Nanodegree: Introduction

These are my personal notes taken while following the [Udacity Computer Vision Nanodegree](https://www.udacity.com/course/computer-vision-nanodegree--nd891).

The nanodegree is composed of six modules:

1. Introduction to Computer Vision
2. Cloud Computing (Optional)
3. Advanced Computer Vision and Deep Learning
4. Object Tracking and Localization
5. Extra Topics: C++ Programming

Each module has a folder with its respective notes.
This folder/file refers to the **third** module: **Advanced Computer Vision and Deep Learning**.

Note that:

- I made many hand-written nortes, which I will scan and push to this repostory.
- I forked the Udacity repositors for the exercises; all the material and  notebooks are there:
	- [CVND_Exercises](https://github.com/mxagar/CVND_Exercises)
	- [DL_PyTorch](https://github.com/mxagar/DL_PyTorch)

Mikel Sagardia, 2022.
No guarantees.

## Practical Installation Notes

I basically followed the installation & setup guide from [CVND_Exercises](https://github.com/mxagar/CVND_Exercises), which can be summarized with the following commands:

```bash
# Create new conda environment to be used for the nanodegree
conda create -n cvnd python=3.6
conda activate cvnd
conda install pytorch torchvision -c pytorch
conda install pip
#conda install -c conda-forge jupyterlab
# Go to the folder where the Udacity DL exercises are cloned, after forking the original repo
cd ~/git_repositories/CVND_Exercises
pip install -r requirements.txt
# I had some issues with numpy and torch
pip uninstall numpy
pip uninstall mkl-service
pip install numpy
pip install mkl-service
```

## Overview of Contents

1. Advanced CNN Architectures
2. YOLO: You Only Look Once
3. Recursive Neural Networks (RNN)
4. Long Short-Term Memory Networks (LSTM)
5. Hyperparameters
6. Attention Mechanisms
7. Image Captioning
8. Project: Image Captioning

## 1. Advanced CNN Architectures

Common CNN architectures defined so far apply convolutional layers to an image with an object to obtain a feature vector that is processed by fully connected layer to yield class probabilities.

![CNNs](./pics/CNN.jpg)

However, real world images contain several objects that overlap and we want to locate them on the same image. Additionally, visual information can be related to text or language information, too. This section deals with architectures that allow that:

- Object detection: R-CNN, YOLO; they detect the bounding boxes of the objects and the they classify them.
- Recurrect Neural Networks (RNN): LSTM; they are able to predict sequences of objects, like text.

### 1.1 Object Detection: Localization

Object detection is basically object **localization** in an image: a bounding box is predicted around the object, together with its class. The bounding box is defined with its center and size: `(x,y,w,h)`. By working with bounding box centers and sizes, we can estimate the distances between the objects (useful for self-driving cars) or the relative semantic postion: behind, in, etc.

One way of predicting bounding boxes consists in adding additional fully connected layers to the feature vector generated by the convolutional layers so that the tuple `(x,y,w,h)` is predicted by regression.

![Object Detection CNNs](./pics/ObjectDetection_CNN.jpg)


### 1.2 Bounding Boxes and Regression

While **cross-entropy loss** is good for classification, regression requires other loss or error computations:

- L1 loss: `(1/n) * sum(abs(y_i - y_pred_i))`
- L2 loss, or (root) mean squared error: `(1/n) * sum((y_i - y_pred_i)^2)`; note that large errors are amplified.
- [Smoooth L1 loss](https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html#torch.nn.SmoothL1Loss): for large errors, L1 is used, for small errors, L2; thus, the goal is to avoid amplifying large errors. See the documentation.

Note that the regression problem of predicting a bounding box `(x,y,w,h)` can be extended to any fixed point cloud prediction, for instance: [face keypoint detection](https://github.com/mxagar/P1_Facial_Keypoints) or human pose estimation.

![Human pose estimation](./pics/human_pose_estimation.png)

In the case of object detection, we now have two losses:

1. Classification loss, e.g., cross-entropy.
2. Regression loss, e.g., L1.

We need to use them both to perform backpropagation; this is often done by summing them up witha weighting factor, which is an added hyperparameter:

`loss = w_c *  classification_loss + w_r * regression_loss`

An option would be to have `w_c = w_r = 0.5`; however, it is usually considered as a hyperparaneter
 to be discovered with the cross-validation split.

### 1.3 Region Proposals: R-CNN

Another challenge in object detection is that we have a variable number of outputs; we don't know the exact number of boxes / objects in the image beforehand. A naive approach would consist in sweeping a window in the image to analyze many cropped image patches, however, that is very expensive.

Instead, **region proposals** were introduced: tradictional image processing techniques are used to identify regions that:

- have many edges
- have same texture
- are enclosed by teh same boundary
- etc.

Thus, with region proposal algorithms we get a set of possible object regions, aka **Regions of Interest (ROI**); many are noise, but the objects will be found there, too!

![Region proposals](./pics/region_proposals.png)

The least sophisticated architecture for object detection is **R-CNN, Region-CNN**: for every input ROI from the region proposal algorithm, it outputs (1) a bounding box and (2) class scores. R-CNNs have  a class called `background`, which is associated for any ROIs containing noise.

Cropped input ROIs need to be processed as always (resize/warp to a standard size, etc.). Note that ROIs are already a bounding box, but the real bounding box does not need to be exactly that of the cropping.

Even though the R-CNN is better than the naive approach with the sliding window, it still is slow, since many ROIs need to be processed for often few objects in the scene.

### 1.4 Fast R-CNN

Instead of feeding each cropped ROI to the CNN individually, Fast R-CNNs convolute the entire image once and project the ROIs on the last set of feature maps before the fully connected layers.

Then, each ROI is warped to a fixed size using **ROI pooling** so that it can be fed to the fully connected layers, which, as before: (1) classify the object and (2) regress the position of its bounding box.

![Fast R-CNN](./pics/Fast_RCNN.png)

**ROI pooling** takes in a rectangular region of any size, performs a maxpooling operation on that region in pieces such that the output is a fixed shape.

Nice animation of how ROI pooling works from [deppsense.ai](https://blog.deepsense.ai/region-of-interest-pooling-explained/):

![ROI pooling](./pics/roi_pooling.gif)

Fast R-CNN is 10x faster to train than R-CNN; its test time is dominated by the suggestion of ROIs.

### 1.5 Faster R-CNN

A way of speeding up the Fast R-CNN consists in using a separate network that suggests ROIs; in other words, we attack the bottleneck we had beforehand.

The steps are the following:

- The image is convoluted until a given layer, which produces a set of feature maps.
- Instead of projecting ROIs on those feature maps, we feed them to a separate network which predicts possible ROIs: these are called **Region Proposal Networks**; if edges or other relevant features have been detected, ROIs that enclose them will emerge.
- The ROI proposals are passed to the original network, which performs a quick binary check: does the ROI contain an object or not? If so, the ROI is taken.
- After that, the network continues as in Fast R-CNN: ROI pooling is applied and a fully connected network predicts classes an bounding boxes.

Thus, the difference is in the ROIs: instead of projecting them after applying classical algorithms, a Region Proposal Network is used with the feature maps. The Faster R-CNNs are the fastest R-CNN networks.

#### Region Proposal Networks

How do the Region Proposal Networks (RPN) from Faster R-CNN work?

They work in a similar fashion as the YOLO network does; see the next section on [YOLO](#-2.-YOLO:-You-Only-Look-Once).

The basic principle is as follows:

- A small (usually 3x3) window is slided on the feature maps. I understand that 3x3 means the stride in both axes, since I don't see how the content of the window is used otherwise.
- `k` anchor boxes are applied on each window. These anchor boxes are predefined boxes with different aspect ratios.
- For each `k` boxes in each window, the probability of it containing an object is measured. If it's higher than a threshold, the anchor box is suggested as a ROI.

During training, the ground truth is given by the real boudning box: if the suggested ROI overlaps considerably with a true bounding box, the suggestion is correct.

Very interesting link on the topic: [Deep Learning for Object Detection: A Comprehensive Review](https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9)


A Github repo witha peer reviewed implementation of the Faster R-CNN: [
faster-rcnn.pytorch](https://github.com/jwyang/faster-rcnn.pytorch)

## 2. YOLO: You Only Look Once

There are some alternatives to R-CNN networks that came out around the same time:

- YOLO: You Only Look Once
- SSD: Single Shot Detection

This section analyzes YOLO.